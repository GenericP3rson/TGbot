,words,messages,ids,urls
0,Gotcha,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
1,:slight_smile:,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
2,Yeah,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
3,-,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
4,I,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
5,was,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
6,specifically,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
7,looking,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
8,for,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
9,a,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
10,way,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
11,to,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
12,"""debug""","Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
13,each,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
14,iteration,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
15,of,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
16,an,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
17,"algorithm,","Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
18,so,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
19,I,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
20,wanted,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
21,to,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
22,inspect,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
23,how,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
24,each,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
25,iteration,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
26,effects,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
27,the,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
28,aggregators.,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
29,Thanks,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
30,a,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
31,lot,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
32,for,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
33,your,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
34,time,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
35,@rik,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
36,and,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
37,@mingxiwu!,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
38,I,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
39,really,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
40,appreciate,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
41,it!,"Gotcha :slight_smile:
Yeah - I was specifically looking for a way to ""debug"" each iteration of an algorithm, so I wanted to inspect how each iteration effects the aggregators.

Thanks a lot for your time @rik and @mingxiwu!
I really appreciate it!",1796,https://community.tigergraph.com/t/debug-printing-vertices-content/624
42,This,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
43,was,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
44,really,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
45,"interesting,","This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
46,as,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
47,it,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
48,demonstrates,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
49,an,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
50,enhancement,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
51,in,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
52,2.6,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
53,that,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
54,I,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
55,"missed,","This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
56,but,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
57,it,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
58,really,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
59,makes,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
60,some,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
61,things,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
62,much,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
63,more,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
64,elegant.,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
65,You,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
66,can,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
67,apply,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
68,a,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
69,vertex,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
70,set,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
71,as,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
72,the,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
73,target,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
74,now.,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
75,If,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
76,you,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
77,use,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
78,the,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
79,global,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
80,accumulator,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
81,"as-is,","This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
82,then,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
83,it,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
84,assumes,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
85,it,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
86,is,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
87,a,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
88,list,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
89,of,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
90,strings,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
91,defining,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
92,a,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
93,set,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
94,of,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
95,types,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
96,(which,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
97,was,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
98,an,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
99,enhancement,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
100,so,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
101,we,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
102,could,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
103,parameterise,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
104,the,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
105,types,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
106,for,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
107,our,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
108,standard,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
109,algorithms).,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
110,"But,","This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
111,if,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
112,you,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
113,convert,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
114,the,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
115,accumulator,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
116,to,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
117,a,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
118,vertex,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
119,set,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
120,using,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
121,the,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
122,curly-bracket,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
123,"syntax,","This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
124,then,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
125,you,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
126,can,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
127,use,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
128,it,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
129,directly.,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
130,This,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
131,fragment,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
132,is,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
133,legal,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
134,syntax:,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
135,SetAccum<Vertex>,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
136,@@target;,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
137,SetAccum<Vertex>,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
138,@@source;,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
139,source_vset,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
140,=,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
141,{@@source};,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
142,target_vset,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
143,=,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
144,{@@target};,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
145,frontier,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
146,=,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
147,SELECT,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
148,u,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
149,FROM,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
150,source_vset:u-()-target_vset:t;,"This was really interesting, as it demonstrates an enhancement in 2.6 that I missed, but it really makes some things much more elegant. 

You can apply a vertex set as the target now. If you use the global accumulator as-is, then it assumes it is a list of strings defining a set of types (which was an enhancement so we could parameterise the types for our standard algorithms).

But, if you convert the accumulator to a vertex set using the curly-bracket syntax, then you can use it directly. This fragment is legal syntax:

    SetAccum<Vertex> @@target;
    SetAccum<Vertex> @@source;
    	
    source_vset  = {@@source};
    target_vset = {@@target};

    frontier = SELECT u
    FROM source_vset:u-()-target_vset:t;",1795,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
151,Just,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
152,to,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
153,finish,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
154,off.,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
155,Nope.,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
156,I,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
157,wasn't,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
158,suggesting,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
159,the,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
160,log,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
161,"approach,","Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
162,if,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
163,you,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
164,are,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
165,specifically,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
166,looking,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
167,to,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
168,apply,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
169,the,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
170,logging,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
171,"function,","Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
172,then,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
173,MingXi's,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
174,solution,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
175,is,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
176,the,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
177,only,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
178,way,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
179,I,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
180,know,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
181,of.,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
182,Given,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
183,your,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
184,routine,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
185,doesn't,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
186,return,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
187,any,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
188,"values,","Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
189,I,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
190,meant,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
191,to,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
192,use,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
193,the,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
194,actual,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
195,print,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
196,function,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
197,after,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
198,the,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
199,select,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
200,block,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
201,was,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
202,complete.,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
203,That,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
204,works,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
205,fine,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
206,with,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
207,vset's.,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
208,CREATE,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
209,QUERY,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
210,test(Vertex,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
211,Src),"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
212,FOR,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
213,GRAPH,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
214,fraud_detection_network,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
215,{,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
216,BOOL,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
217,debug,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
218,=,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
219,TRUE;,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
220,ListAccum<Vertex>,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
221,@@log_vertices;,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
222,start,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
223,=,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
224,{Src};,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
225,WHILE,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
226,start.size(),"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
227,>,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
228,0,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
229,limit,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
230,10,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
231,DO,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
232,start,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
233,=,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
234,SELECT,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
235,v,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
236,FROM,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
237,start:,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
238,s,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
239,-,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
240,(:e),"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
241,->,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
242,:v,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
243,POST-ACCUM,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
244,@@log_vertices,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
245,+=,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
246,v,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
247,END;,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
248,myVset,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
249,=,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
250,{@@log_vertices};,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
251,print,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
252,myVset;,"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
253,},"Just to finish off. Nope. I wasn't suggesting the log approach, if you are specifically looking to apply the logging function, then MingXi's solution is the only way I know of.

Given your routine doesn't return any values, I meant to use the actual print function after the select block was complete. That works fine with vset's.

    CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v
	END;

        myVset = {@@log_vertices};
        print myVset;

    }",1794,https://community.tigergraph.com/t/debug-printing-vertices-content/624
254,"Yeah,","Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
255,I,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
256,was,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
257,trying,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
258,to,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
259,avoid,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
260,that...,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
261,:smiley:,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
262,But,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
263,if,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
264,there's,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
265,no,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
266,way,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
267,to,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
268,print,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
269,the,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
270,whole,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
271,content,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
272,of,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
273,the,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
274,vertex,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
275,-,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
276,I'll,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
277,do,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
278,that!,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
279,Thanks!,"Yeah, I was trying to avoid that... :smiley:
But if there's no way to print the whole content of the vertex - I'll do that! Thanks!",1793,https://community.tigergraph.com/t/debug-printing-vertices-content/624
280,"[quote=""dsolow,","[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
281,"post:1,","[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
282,"topic:627""]","[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
283,```,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
284,SetAccum<Vertex>,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
285,@@target;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
286,SetAccum<Vertex>,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
287,@@source;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
288,source,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
289,(Any),"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
290,=,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
291,@@source;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
292,frontier,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
293,=,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
294,SELECT,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
295,u,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
296,FROM,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
297,source:u-(shareholder_of:e)->(@@target):v;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
298,```,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
299,[/quote],"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
300,You,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
301,might,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
302,try,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
303,this,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
304,for,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
305,your,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
306,goal.,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
307,SetAccum<Vertex>,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
308,@@target;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
309,SetAccum<Vertex>,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
310,@@source;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
311,source,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
312,(Any),"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
313,=,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
314,@@source;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
315,frontier,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
316,=,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
317,SELECT,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
318,u,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
319,FROM,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
320,source:u,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
321,-,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
322,(shareholder_of:e),"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
323,->,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
324,:v,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
325,WHERE,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
326,v,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
327,in,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
328,@@target,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
329,ACCUM,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
330,xx;,"[quote=""dsolow, post:1, topic:627""]
```
SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```
[/quote]

You might try this for your goal. 

SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	               FROM source:u - (shareholder_of:e) -> :v
                       WHERE v in @@target
                       ACCUM xx;",1792,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
331,"[quote=""dsolow,","[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
332,"post:1,","[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
333,"topic:627""]","[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
334,```,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
335,a,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
336,global,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
337,SetAcum,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
338,accumulator,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
339,containing,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
340,a,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
341,set,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
342,of,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
343,edges,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
344,or,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
345,vertices,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
346,"(""@@""accumName)","[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
347,```,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
348,[/quote],"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
349,It's,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
350,a,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
351,doc,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
352,bug.,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
353,I,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
354,have,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
355,filed,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
356,a,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
357,ticket,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
358,internally.,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
359,In,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
360,syntax,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
361,"V2,","[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
362,3.0,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
363,we,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
364,support,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
365,this,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
366,feature.,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
367,https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation,"[quote=""dsolow, post:1, topic:627""]
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```
[/quote]

It's a doc bug. I have filed a ticket internally. 

In syntax V2, 3.0 we support this feature.
https://docs.tigergraph.com/start/gsql-102/multiple-hop-and-accumulation",1791,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
368,"[quote=""DenisK,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
369,"post:3,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
370,"topic:624""]","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
371,ur,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
372,suggestion,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
373,wouldnt,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
374,[/quote],"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
375,you,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
376,can,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
377,try,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
378,to,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
379,spell,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
380,out,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
381,the,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
382,expressions.,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
383,E.g.,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
384,ACCUM,"[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
385,"log(true,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
386,"s.id,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
387,"s.@SampledEdges1,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
388,"s.@randomfactorList1,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
389,"s.@ModMap1,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
390,"randomfactor,","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
391,"s.@ModMap1.get(randomfactor),","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
392,"sampledEdges),","[quote=""DenisK, post:3, topic:624""]
ur suggestion wouldnt
[/quote]

you can try to spell out the expressions. E.g. 

ACCUM
 log(true, s.id, s.@SampledEdges1, s.@randomfactorList1, s.@ModMap1, randomfactor, s.@ModMap1.get(randomfactor), sampledEdges),",1790,https://community.tigergraph.com/t/debug-printing-vertices-content/624
393,I'm,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
394,using,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
395,2.6.1.,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
396,In,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
397,the,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
398,docs,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
399,I,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
400,see,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
401,the,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
402,following:,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
403,https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
404,This,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
405,indicates,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
406,that,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
407,I,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
408,can,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
409,use,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
410,a,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
411,vertex,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
412,set,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
413,accum,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
414,like,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
415,this:,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
416,```,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
417,SetAccum<Vertex>,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
418,@@target;,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
419,SetAccum<Vertex>,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
420,@@source;,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
421,source,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
422,(Any),"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
423,=,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
424,@@source;,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
425,frontier,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
426,=,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
427,SELECT,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
428,u,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
429,FROM,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
430,source:u-(shareholder_of:e)->(@@target):v;,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
431,```,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
432,So,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
433,basically,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
434,I'm,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
435,looking,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
436,for,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
437,edges,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
438,between,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
439,two,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
440,sets.,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
441,"However,","I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
442,this,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
443,gives,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
444,me,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
445,the,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
446,following,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
447,error:,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
448,```,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
449,"(9,","I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
450,38),"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
451,Error:,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
452,@@target,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
453,is,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
454,not,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
455,string,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
456,or,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
457,set,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
458,of,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
459,string,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
460,expression,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
461,```,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
462,This,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
463,seems,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
464,to,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
465,indicate,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
466,that,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
467,only,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
468,a,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
469,set,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
470,of,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
471,strings,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
472,is,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
473,"supported,","I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
474,even,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
475,though,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
476,the,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
477,docs,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
478,claim,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
479,a,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
480,set,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
481,of,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
482,vertices,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
483,is,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
484,supported:,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
485,```,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
486,a,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
487,global,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
488,SetAcum,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
489,accumulator,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
490,containing,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
491,a,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
492,set,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
493,of,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
494,edges,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
495,or,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
496,vertices,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
497,"(""@@""accumName)","I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
498,```,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
499,Any,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
500,idea,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
501,what's,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
502,wrong,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
503,here?,"I'm using 2.6.1. In the docs I see the following: https://docs.tigergraph.com/v/2.6/dev/gsql-ref/querying/select-statement#edge-set-and-target-vertex-set-options

This indicates that I can use a vertex set accum like this:

```
	SetAccum<Vertex> @@target;
	SetAccum<Vertex> @@source;
	
	source (Any) = @@source;
	
	frontier = SELECT u
	  FROM source:u-(shareholder_of:e)->(@@target):v;
```

So basically I'm looking for edges between two sets. However, this gives me the following error:
```
(9, 38) Error: @@target is not string or set of string expression
```

This seems to indicate that only a set of strings is supported, even though the docs claim a set of vertices is supported: 
```
a global SetAcum accumulator containing a set of edges or vertices
(""@@""accumName)
```

Any idea what's wrong here?",1789,https://community.tigergraph.com/t/using-vertex-set-accum-as-target-set/627
504,Mmmm...,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
505,I,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
506,think,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
507,I'm,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
508,missing,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
509,something...,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
510,:slight_smile:,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
511,Do,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
512,you,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
513,mean,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
514,something,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
515,like,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
516,this?,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
517,```,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
518,CREATE,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
519,QUERY,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
520,test(Vertex,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
521,Src),"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
522,FOR,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
523,GRAPH,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
524,fraud_detection_network,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
525,{,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
526,BOOL,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
527,debug,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
528,=,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
529,TRUE;,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
530,ListAccum<Vertex>,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
531,@@log_vertices;,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
532,start,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
533,=,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
534,{Src};,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
535,WHILE,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
536,start.size(),"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
537,>,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
538,0,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
539,limit,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
540,10,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
541,DO,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
542,start,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
543,=,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
544,SELECT,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
545,v,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
546,FROM,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
547,start:,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
548,s,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
549,-,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
550,(:e),"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
551,->,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
552,:v,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
553,POST-ACCUM,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
554,@@log_vertices,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
555,+=,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
556,"v,","Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
557,vset,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
558,=,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
559,"{@@log_vertices},","Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
560,"LOG(debug,vset);","Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
561,END;,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
562,},"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
563,```,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
564,When,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
565,I,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
566,tried,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
567,"it,","Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
568,I've,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
569,received,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
570,the,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
571,following,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
572,error,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
573,message:,"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
574,"![image|690x154,","Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
575,100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png),"Mmmm... I think I'm missing something... :slight_smile:

 Do you mean something like this?
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	BOOL debug = TRUE;
	ListAccum<Vertex> @@log_vertices;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    POST-ACCUM
	      @@log_vertices += v,
	      vset = {@@log_vertices},
	      LOG(debug,vset);
	END;
}
```

When I tried it, I've received the following error message:
![image|690x154, 100%](upload://fb9oxT7eXOlktpnFLV1MAsgfylu.png)",1788,https://community.tigergraph.com/t/debug-printing-vertices-content/624
576,It,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
577,should,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
578,work,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
579,fine,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
580,within,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
581,ACCUM/POST-ACCUM.,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
582,Unlike,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
583,"print,","It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
584,which,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
585,doesn't,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
586,(unless,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
587,you,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
588,are,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
589,printing,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
590,to,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
591,file).,"It should work fine within ACCUM/POST-ACCUM. Unlike print, which doesn't (unless you are printing to file).",1786,https://community.tigergraph.com/t/debug-printing-vertices-content/624
592,Thanks,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
593,for,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
594,the,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
595,"response,","Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
596,@rik!,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
597,If,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
598,I'm,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
599,not,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
600,"mistaken,","Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
601,your,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
602,suggestion,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
603,wouldn't,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
604,work,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
605,inside,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
606,ACCUM,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
607,or,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
608,POST-ACCUM,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
609,clause...,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
610,Right?,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
611,I'm,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
612,interested,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
613,in,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
614,looking,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
615,into,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
616,what's,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
617,happening,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
618,during,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
619,each,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
620,iteration,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
621,of,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
622,the,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
623,WHILE,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
624,loop.,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
625,That's,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
626,why,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
627,I'm,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
628,using,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
629,the,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
630,LOG,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
631,command...,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
632,Any,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
633,idea,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
634,if,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
635,that's,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
636,possible?,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
637,:slight_smile:,"Thanks for the response, @rik!

If I'm not mistaken, your suggestion wouldn't work inside ACCUM or POST-ACCUM clause... Right?

I'm interested in looking into what's happening during each iteration of the WHILE loop.
That's why I'm using the LOG command...

Any idea if that's possible? :slight_smile:",1785,https://community.tigergraph.com/t/debug-printing-vertices-content/624
638,"So,","So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
639,the,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
640,way,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
641,to,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
642,do,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
643,this,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
644,I,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
645,use,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
646,is,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
647,to,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
648,create,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
649,a,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
650,global,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
651,accumulator,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
652,and,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
653,add,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
654,the,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
655,relevant,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
656,vertices,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
657,to,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
658,"that,","So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
659,say:,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
660,>,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
661,ListAccum,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
662,<VERTEX>,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
663,@@log_vertices,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
664,At,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
665,the,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
666,end,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
667,I'll,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
668,convert,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
669,that,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
670,into,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
671,a,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
672,vertex,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
673,set,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
674,and,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
675,then,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
676,print,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
677,that:,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
678,>,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
679,vset,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
680,=,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
681,{@@log_vertices};,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
682,>,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
683,print,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
684,vset;,"So, the way to do this I use is to create a global accumulator and add the relevant vertices to that, say:

> ListAccum <VERTEX> @@log_vertices

At the end I'll convert that into a vertex set and then print that:

> vset = {@@log_vertices};
> print vset;",1784,https://community.tigergraph.com/t/debug-printing-vertices-content/624
685,In,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
686,order,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
687,to,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
688,debug,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
689,a,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
690,query,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
691,that,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
692,I'm,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
693,"writing,","In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
694,I'd,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
695,like,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
696,to,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
697,be,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
698,able,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
699,to,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
700,display,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
701,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
702,content,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
703,of,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
704,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
705,vertices,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
706,that,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
707,have,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
708,been,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
709,traversed.,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
710,I'm,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
711,using,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
712,something,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
713,like,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
714,this:,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
715,```,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
716,CREATE,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
717,QUERY,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
718,test(Vertex,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
719,Src),"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
720,FOR,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
721,GRAPH,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
722,fraud_detection_network,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
723,{,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
724,BOOL,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
725,debug,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
726,=,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
727,TRUE;,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
728,start,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
729,=,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
730,{Src};,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
731,WHILE,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
732,start.size(),"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
733,>,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
734,0,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
735,limit,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
736,10,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
737,DO,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
738,start,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
739,=,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
740,SELECT,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
741,v,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
742,FROM,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
743,start:,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
744,s,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
745,-,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
746,(:e),"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
747,->,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
748,:v,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
749,ACCUM,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
750,//,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
751,some,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
752,logic...,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
753,POST-ACCUM,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
754,"LOG(debug,v);","In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
755,END;,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
756,},"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
757,```,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
758,The,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
759,problem,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
760,is,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
761,that,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
762,it,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
763,only,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
764,prints,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
765,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
766,ID,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
767,of,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
768,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
769,"vertex,","In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
770,which,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
771,is,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
772,"expected,","In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
773,if,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
774,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
775,behavior,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
776,of,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
777,LOG,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
778,is,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
779,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
780,same,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
781,as,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
782,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
783,behavior,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
784,of,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
785,PRINT,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
786,-,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
787,https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
788,Is,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
789,there,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
790,a,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
791,way,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
792,to,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
793,print,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
794,the,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
795,"""whole","In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
796,"content""","In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
797,(i.e.,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
798,attributes,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
799,and,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
800,accumulators),"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
801,of,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
802,a,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
803,vertex?,"In order to debug a query that I'm writing, I'd like to be able to display the content of the vertices that have been traversed.
I'm using something like this:
```
CREATE QUERY test(Vertex Src) FOR GRAPH fraud_detection_network { 
	
	BOOL debug = TRUE;
 
	start = {Src};
	WHILE start.size() > 0 limit 10 DO 
	  start = SELECT v 
	    FROM start: s - (:e) -> :v
	    ACCUM
	      // some logic...
	    POST-ACCUM
	      LOG(debug,v);
	END;
}
```
The problem is that it only prints the ID of the vertex, which is expected, if the behavior of LOG is the same as the behavior of PRINT - https://docs.tigergraph.com/dev/gsql-ref/querying/output-statements-and-file-objects#json-format-values.

Is there a way to print the ""whole content"" (i.e. attributes and accumulators) of a vertex?",1782,https://community.tigergraph.com/t/debug-printing-vertices-content/624
804,"[quote=""Jon_Herke,","[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
805,"post:3,","[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
806,"topic:617""]","[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
807,GSQL,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
808,[/quote],"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
809,great.,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
810,we,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
811,are,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
812,schema,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
813,first,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
814,database.,"[quote=""Jon_Herke, post:3, topic:617""]
GSQL
[/quote]

great. we are schema first database.",1777,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
815,@mingxiwu,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
816,we,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
817,figured,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
818,it,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
819,out.,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
820,You,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
821,need,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
822,to,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
823,**add,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
824,the,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
825,edge,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
826,to,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
827,the,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
828,schema**,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
829,before,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
830,you,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
831,can,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
832,use,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
833,the,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
834,INSERT,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
835,of,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
836,an,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
837,edge,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
838,in,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
839,your,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
840,GSQL.,@mingxiwu we figured it out. You need to **add the edge to the schema** before you can use the INSERT of an edge in your GSQL.,1776,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
841,the,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
842,doc,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
843,of,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
844,shortestpath,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
845,is,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
846,a,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
847,built-in,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
848,query.,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
849,It's,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
850,not,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
851,written,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
852,by,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
853,GSQL.,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
854,If,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
855,you,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
856,use,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
857,GSQL,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
858,install,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
859,query,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
860,to,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
861,generate,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
862,the,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
863,"endpoint,","the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
864,we,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
865,do,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
866,not,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
867,support,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
868,passing,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
869,parameter,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
870,by,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
871,json,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
872,using,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
873,POST,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
874,yet.,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
875,It's,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
876,coming,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
877,though.,"the doc of shortestpath is a built-in query. It's not written by GSQL. If you use GSQL install query to generate the endpoint, we do not support passing parameter by json using POST yet. It's coming though.",1775,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
878,What,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
879,about,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
880,the,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
881,example,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
882,right,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
883,here:,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
884,https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
885,Looks,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
886,like,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
887,it's,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
888,using,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
889,POST,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
890,data,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
891,payload,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
892,to,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
893,specify,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
894,vertices:,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
895,```,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
896,curl,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
897,-s,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
898,-X,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
899,POST,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
900,'http://localhost:9000/shortestpath',"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
901,-d,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
902,'{,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
903,"""sources"":[{""type"":""VidUser"",""id"":""2""}],","What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
904,"""targets"":[{""type"":""VidUser"",""id"":""0""},","What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
905,"{""type"":""VidUser"",""id"":""3""}],","What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
906,"""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating","What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
907,>,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
908,5,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
909,and,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
910,date_time,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
911,>,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
912,"1000}],","What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
913,"""maxLength"":4","What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
914,}',"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
915,```,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
916,Also,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
917,looks,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
918,like,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
919,there's,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
920,a,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
921,missing,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
922,quote,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
923,in,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
924,that,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
925,example.,"What about the example right here: https://docs.tigergraph.com/dev/restpp-api/built-in-endpoints#post-shortestpath-graphname-shortest-path-search

Looks like it's using POST data payload to specify vertices:
```
curl -s -X POST 'http://localhost:9000/shortestpath' -d
'{
  ""sources"":[{""type"":""VidUser"",""id"":""2""}],
  ""targets"":[{""type"":""VidUser"",""id"":""0""}, {""type"":""VidUser"",""id"":""3""}],
  ""edgeFilters"":[{""type"":""User_Video"",""condition"":""rating > 5 and date_time > 1000}],
  ""maxLength"":4
}'
```

Also looks like there's a missing quote in that example.",1774,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
926,If,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
927,the,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
928,REST++,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
929,server,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
930,is,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
931,"local,","If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
932,then,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
933,server_ip,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
934,is,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
935,localhost.,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
936,The,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
937,request,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
938,can,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
939,use,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
940,either,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
941,the,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
942,GET,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
943,or,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
944,POST,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
945,method.,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
946,The,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
947,query,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
948,parameter,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
949,values,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
950,are,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
951,either,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
952,included,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
953,directly,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
954,in,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
955,the,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
956,query,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
957,string,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
958,of,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
959,the,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
960,HTTP,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
961,request's,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
962,URL,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
963,or,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
964,supplied,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
965,using,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
966,a,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
967,data,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
968,payload.,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
969,The,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
970,basic,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
971,format,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
972,for,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
973,the,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
974,parameters,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
975,as,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
976,a,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
977,query,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
978,string,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
979,is,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
980,param1=value&param2=value&param3=value,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
981,If,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
982,you,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
983,use,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
984,data,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
985,"payload,","If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
986,JSON,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
987,format,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
988,only,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
989,support,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
990,GET.,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
991,We,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
992,are,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
993,developing,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
994,JSON,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
995,format,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
996,to,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
997,support,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
998,POST,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
999,in,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1000,the,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1001,next,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1002,minor,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1003,release,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1004,version.,"If the REST++ server is local, then server_ip is localhost. 

The request can use either the GET or POST method. The query parameter values are either included directly in the query string of the HTTP request's URL or supplied using a data payload. The basic format for the parameters as a query string is
param1=value&param2=value&param3=value

If you use data payload, JSON format only support GET. We are developing JSON format to support POST in the next minor release version.",1773,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1005,what's,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1006,the,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1007,schema,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1008,of,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1009,KNOWS?,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1010,Based,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1011,on,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1012,the,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1013,tutorial,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1014,https://docs.tigergraph.com/start/gsql-102/adv/dml,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1015,The,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1016,VALUES,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1017,clause,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1018,in,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1019,your,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1020,INSERT,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1021,statement,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1022,should,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1023,share,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1024,the,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1025,same,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1026,schema,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1027,of,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1028,KNOWS,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1029,edge.,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1030,Please,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1031,post,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1032,your,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1033,DDL,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1034,of,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1035,the,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1036,graph,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1037,"schema,","what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1038,and,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1039,the,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1040,error,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1041,you,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1042,are,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1043,getting.,"what's the schema of KNOWS?

Based on the tutorial 
https://docs.tigergraph.com/start/gsql-102/adv/dml
The VALUES clause in your INSERT statement should share the same schema of KNOWS edge. 

Please post your DDL of the graph schema, and the error you are getting.",1772,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1044,Having,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1045,a,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1046,bit,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1047,of,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1048,trouble,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1049,figuring,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1050,out,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1051,how,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1052,to,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1053,pass,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1054,a,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1055,Vertex,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1056,parameter,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1057,via,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1058,HTTP,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1059,POST.,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1060,Here's,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1061,the,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1062,function,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1063,signature:,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1064,`CREATE,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1065,QUERY,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1066,delete_strategy_instance(VERTEX<IbisInstance>,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1067,instance),"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1068,syntax,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1069,v2,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1070,{`,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1071,I'm,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1072,doing,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1073,a,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1074,POST,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1075,with,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1076,"`{""instance"":","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1077,"""test_delete_a1""}`","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1078,And,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1079,I've,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1080,also,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1081,tried,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1082,"`{""instance"":","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1083,"{""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1084,And,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1085,I've,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1086,also,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1087,tried,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1088,"`{""instance"":","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1089,"{""type"":""IbisInstance"",""id"":""test_delete_a1""}}`","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1090,Using,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1091,GET,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1092,"works,","Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1093,but,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1094,I,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1095,want,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1096,to,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1097,make,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1098,sure,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1099,I,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1100,know,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1101,how,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1102,to,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1103,get,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1104,this,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1105,working,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1106,with,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1107,POST,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1108,for,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1109,future.,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1110,Any,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1111,advice?,"Having a bit of trouble figuring out how to pass a Vertex parameter via HTTP POST.

Here's the function signature:

`CREATE QUERY delete_strategy_instance(VERTEX<IbisInstance> instance) syntax v2 {`

I'm doing a POST with `{""instance"": ""test_delete_a1""}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""instance_name"":""test_delete_a1""}}`
And I've also tried `{""instance"": {""type"":""IbisInstance"",""id"":""test_delete_a1""}}`

Using GET works, but I want to make sure I know how to get this working with POST for future.
Any advice?",1771,https://community.tigergraph.com/t/how-to-pass-vertex-id-as-a-parameter-via-post/620
1112,It;s,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1113,worth,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1114,noting,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1115,the,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1116,technique,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1117,here.,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1118,Static,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1119,accumulators,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1120,are,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1121,used,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1122,as,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1123,a,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1124,"cache,","It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1125,and,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1126,retain,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1127,their,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1128,contents,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1129,across,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1130,invocations.,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1131,The,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1132,weights,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1133,here,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1134,are,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1135,being,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1136,held,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1137,in,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1138,a,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1139,"file,","It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1140,the,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1141,scoring,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1142,routine,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1143,itself,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1144,is,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1145,expressed,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1146,as,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1147,a,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1148,user-defined,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1149,query.,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1150,The,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1151,weights,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1152,here,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1153,are,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1154,loaded,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1155,as,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1156,a,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1157,"file,","It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1158,but,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1159,could,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1160,just,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1161,as,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1162,easily,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1163,be,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1164,held,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1165,in,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1166,the,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1167,database,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1168,and,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1169,loaded,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1170,from,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1171,there.,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1172,Ref,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1173,my,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1174,current,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1175,ML,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1176,"work,","It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1177,I'm,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1178,looking,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1179,at,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1180,how,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1181,we,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1182,can,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1183,integrate,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1184,catboost,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1185,into,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1186,TG,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1187,and,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1188,load,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1189,trained,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1190,models,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1191,into,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1192,that.,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1193,This,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1194,may,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1195,take,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1196,a,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1197,week,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1198,or,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1199,two,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1200,to,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1201,get,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1202,working.,"It;s worth noting the technique here.

Static accumulators are used as a cache, and retain their contents across invocations.
The weights here are being held in a file, the scoring routine itself is expressed as a user-defined query.

The weights here are loaded as a file, but could just as easily be held in the database and loaded from there.

Ref my current ML work, I'm looking at how we can integrate catboost into TG and load trained models into that. This may take a week or two to get working.",1768,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1203,![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png),"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1204,Hi,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1205,"all,","![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1206,do,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1207,not,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1208,know,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1209,if,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1210,this,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1211,is,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1212,the,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1213,correct,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1214,way,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1215,of,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1216,doing,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1217,"it,","![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1218,but,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1219,I,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1220,am,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1221,currently,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1222,trying,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1223,to,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1224,create,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1225,an,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1226,edge,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1227,between,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1228,two,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1229,vertices,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1230,(of,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1231,different,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1232,types),"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1233,that,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1234,have,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1235,a,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1236,common,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1237,vertex,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1238,between,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1239,them.,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1240,I,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1241,seem,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1242,to,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1243,not,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1244,be,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1245,understanding,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1246,the,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1247,syntax,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1248,correctly,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1249,and,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1250,have,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1251,errors.,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1252,Can,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1253,someone,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1254,point,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1255,me,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1256,in,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1257,the,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1258,right,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1259,direction?,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1260,Thanks,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1261,in,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1262,advance.,"![screenshot|690x388](upload://t4MVpWaqH542tbt6V6jZk1Zkj09.png) 

Hi all, do not know if this is the correct way of doing it, but I am currently trying to create an edge between two vertices (of different types) that have a common vertex between them. I seem to not be understanding the syntax correctly and have errors. Can someone point me in the right direction? Thanks in advance.",1767,https://community.tigergraph.com/t/adding-edge-between-two-vertices-that-are-connected-by-common-vertex/617
1263,@Maatdeamon,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1264,You,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1265,can,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1266,use,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1267,LOADACCUM,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1268,to,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1269,load,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1270,parameter,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1271,files,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1272,into,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1273,a,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1274,global,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1275,accumulator.,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1276,And,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1277,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1278,loaded,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1279,global,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1280,accumulator,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1281,can,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1282,invoke,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1283,an,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1284,expression,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1285,function,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1286,to,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1287,do,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1288,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1289,scoring.,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1290,-,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1291,LOADACCUM,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1292,doc,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1293,is,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1294,here,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1295,https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1296,-,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1297,User-defined,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1298,expression,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1299,function,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1300,doc,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1301,is,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1302,here,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1303,https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1304,E.g.,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1305,below,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1306,is,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1307,a,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1308,query,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1309,we,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1310,used,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1311,to,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1312,do,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1313,scoring.,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1314,TigerGraph,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1315,is,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1316,used,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1317,to,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1318,collect,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1319,graph,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1320,"feature,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1321,and,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1322,we,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1323,got,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1324,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1325,trained,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1326,classifier,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1327,parameter,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1328,from,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1329,external,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1330,ML,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1331,"tools,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1332,and,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1333,load,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1334,them,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1335,into,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1336,global,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1337,"accumulators,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1338,and,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1339,for,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1340,a,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1341,new,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1342,"phone,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1343,we,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1344,collect,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1345,its,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1346,graph,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1347,"features,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1348,and,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1349,called,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1350,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1351,user-define,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1352,scoring,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1353,function,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1354,with,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1355,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1356,classifier,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1357,parameters,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1358,and,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1359,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1360,newly,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1361,collected,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1362,features.,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1363,CREATE,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1364,QUERY,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1365,scoringMethod(vertex<phone>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1366,"phoneId,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1367,string,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1368,inputPath,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1369,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1370,"""/tmp/lgweight.configure"",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1371,string,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1372,internalClusterFileName,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1373,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1374,"""/tmp/kMeanWeight_internal.csv"",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1375,string,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1376,externalClusterFileName,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1377,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1378,"""/tmp/kMeanWeight_external.csv"",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1379,bool,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1380,reloadWeightInfo,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1381,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1382,"false,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1383,string,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1384,configFileName,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1385,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1386,"""/tmp/collectFeatures.config"",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1387,bool,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1388,reloadConfig,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1389,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1390,"false,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1391,int,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1392,numOfTopFriends,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1393,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1394,"5,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1395,float,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1396,abnormalThreshold,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1397,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1398,"0.5,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1399,float,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1400,advertisementThreshold,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1401,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1402,"0.5,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1403,bool,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1404,usePrint,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1405,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1406,true),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1407,for,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1408,graph,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1409,testGraph,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1410,returns,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1411,(ListAccum<float>),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1412,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1413,TYPEDEF,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1414,tuple<float,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1415,"weight,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1416,float,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1417,"scale,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1418,float,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1419,mean>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1420,ParamInfo;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1421,ListAccum<float>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1422,@@featureList;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1423,ListAccum<float>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1424,@@scoreList;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1425,int,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1426,isExternal,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1427,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1428,0;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1429,int,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1430,fraudFlag,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1431,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1432,-1;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1433,static,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1434,ListAccum<ParamInfo>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1435,@@paramInfo0;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1436,static,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1437,ListAccum<ParamInfo>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1438,@@paramInfo1;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1439,static,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1440,ListAccum<ParamInfo>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1441,@@paramInfo2;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1442,static,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1443,ListAccum<ParamInfo>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1444,@@paramInfo3;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1445,static,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1446,ListAccum<ListAccum<float>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1447,>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1448,@@InternalCluster;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1449,static,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1450,ListAccum<ListAccum<float>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1451,>,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1452,@@ExternalCluster;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1453,if,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1454,(@@paramInfo0.size(),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1455,==,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1456,0,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1457,or,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1458,reloadWeightInfo,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1459,==,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1460,true),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1461,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1462,@@paramInfo0.clear();,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1463,@@paramInfo0,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1464,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1465,"{loadAccum(inputPath,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1466,"$0,$1,$2,"","",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1467,false)};,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1468,@@paramInfo1.clear();,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1469,@@paramInfo1,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1470,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1471,"{loadAccum(inputPath,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1472,"$3,$4,$5,"","",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1473,false)};,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1474,@@paramInfo2.clear();,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1475,@@paramInfo2,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1476,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1477,"{loadAccum(inputPath,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1478,"$6,$7,$8,"","",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1479,false)};,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1480,@@paramInfo3.clear();,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1481,@@paramInfo3,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1482,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1483,"{loadAccum(inputPath,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1484,"$9,$10,$11,"","",","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1485,false)};,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1486,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1487,if,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1488,(@@InternalCluster.size(),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1489,==,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1490,0,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1491,or,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1492,reloadWeightInfo,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1493,==,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1494,true),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1495,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1496,@@InternalCluster.clear();,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1497,@@InternalCluster,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1498,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1499,loadClusterInfo(internalClusterFileName);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1500,@@ExternalCluster.clear();,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1501,@@ExternalCluster,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1502,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1503,loadClusterInfo(externalClusterFileName);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1504,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1505,//call,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1506,another,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1507,gsql,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1508,query,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1509,to,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1510,collect,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1511,features,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1512,@@featureList,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1513,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1514,"collectFeaturesR(phoneId,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1515,"configFileName,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1516,"reloadConfig,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1517,"numOfTopFriends,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1518,false);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1519,//obtain,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1520,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1521,last,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1522,element,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1523,of,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1524,featureList,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1525,which,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1526,indicates,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1527,isExternal,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1528,isExternal,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1529,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1530,@@featureList.get(69);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1531,//scoring,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1532,by,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1533,calling,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1534,an,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1535,user,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1536,defined,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1537,expression,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1538,function.,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1539,"score(phoneId,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1540,"@@featureList,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1541,"abnormalThreshold,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1542,"advertisementThreshold,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1543,"@@paramInfo0,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1544,"@@paramInfo1,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1545,"@@paramInfo2,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1546,"@@paramInfo3,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1547,@@scoreList);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1548,//set,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1549,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1550,initial,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1551,scamCluster,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1552,as,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1553,"-1,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1554,which,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1555,means,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1556,not,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1557,a,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1558,valid,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1559,scam,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1560,@@scoreList,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1561,+=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1562,-1;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1563,//Get,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1564,the,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1565,fraud,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1566,flag,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1567,out,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1568,fraudFlag,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1569,=,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1570,@@scoreList.get(0);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1571,if,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1572,(fraudFlag,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1573,==,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1574,3),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1575,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1576,if,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1577,(isExternal,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1578,==,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1579,0),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1580,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1581,"updateScamClusterFlag(@@featureList,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1582,"@@InternalCluster,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1583,@@scoreList);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1584,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1585,else,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1586,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1587,"updateScamClusterFlag(@@featureList,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1588,"@@ExternalCluster,","@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1589,@@scoreList);,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1590,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1591,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1592,if,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1593,(usePrint),"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1594,{,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1595,print,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1596,@@scoreList;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1597,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1598,return,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1599,@@scoreList;,"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1600,},"@Maatdeamon

You can use LOADACCUM to load parameter files into a global accumulator. And the loaded global accumulator can invoke an expression function to do the scoring. 

- LOADACCUM doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/declaration-and-assignment-statements#loadaccum-statement
- User-defined expression function doc is here https://docs.tigergraph.com/dev/gsql-ref/querying/operators-functions-and-expressions#user-defined-functions


E.g.  below is a query we used to do scoring. TigerGraph is used to collect graph feature, and we got the trained classifier parameter from external ML tools, and load them into global accumulators, and for a new phone, we collect its graph features, and called the user-define scoring function with the classifier parameters and the newly collected features. 


CREATE QUERY scoringMethod(vertex<phone> phoneId, string inputPath = ""/tmp/lgweight.configure"", string internalClusterFileName = ""/tmp/kMeanWeight_internal.csv"", string externalClusterFileName = ""/tmp/kMeanWeight_external.csv"", bool reloadWeightInfo = false, string configFileName = ""/tmp/collectFeatures.config"", bool reloadConfig = false, int numOfTopFriends = 5, float abnormalThreshold = 0.5, float advertisementThreshold = 0.5, bool usePrint = true) for graph testGraph returns (ListAccum<float>)
{
  TYPEDEF tuple<float weight, float scale, float mean> ParamInfo;
  ListAccum<float> @@featureList;
  ListAccum<float> @@scoreList;
  int isExternal = 0;
  int fraudFlag = -1;
  static ListAccum<ParamInfo> @@paramInfo0;
  static ListAccum<ParamInfo> @@paramInfo1;
  static ListAccum<ParamInfo> @@paramInfo2;
  static ListAccum<ParamInfo> @@paramInfo3;
  static ListAccum<ListAccum<float> > @@InternalCluster;
  static ListAccum<ListAccum<float> > @@ExternalCluster;

  if (@@paramInfo0.size() == 0 or reloadWeightInfo == true) {
    @@paramInfo0.clear();
    @@paramInfo0 = {loadAccum(inputPath, $0,$1,$2,"","", false)};
    @@paramInfo1.clear();
    @@paramInfo1 = {loadAccum(inputPath, $3,$4,$5,"","", false)};
    @@paramInfo2.clear();
    @@paramInfo2 = {loadAccum(inputPath, $6,$7,$8,"","", false)};
    @@paramInfo3.clear();
    @@paramInfo3 = {loadAccum(inputPath, $9,$10,$11,"","", false)};
  }
  if (@@InternalCluster.size() == 0 or reloadWeightInfo == true) {
    @@InternalCluster.clear();
    @@InternalCluster = loadClusterInfo(internalClusterFileName);
    @@ExternalCluster.clear();
    @@ExternalCluster = loadClusterInfo(externalClusterFileName);
  }
  //call another gsql query to collect features
  @@featureList = collectFeaturesR(phoneId, configFileName, reloadConfig, numOfTopFriends, false); 
  //obtain the last element of featureList which indicates isExternal
  isExternal = @@featureList.get(69);
  //scoring by calling an user defined expression function.
  score(phoneId,
                  @@featureList,
                  abnormalThreshold,
                  advertisementThreshold,
                  @@paramInfo0,
                  @@paramInfo1,
                  @@paramInfo2,
                  @@paramInfo3,
                  @@scoreList);
  //set the initial scamCluster as -1, which means not a valid scam
  @@scoreList += -1;
  //Get the fraud flag out
  fraudFlag = @@scoreList.get(0);
  if (fraudFlag == 3) {
    if (isExternal == 0) {
      updateScamClusterFlag(@@featureList, @@InternalCluster, @@scoreList);
    } else {
      updateScamClusterFlag(@@featureList, @@ExternalCluster, @@scoreList);
    }
  }

  if (usePrint) {
    print @@scoreList;
  }
  return @@scoreList;
}",1764,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1601,Thanks,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1602,a,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1603,lot,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1604,@Bruno,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1605,for,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1606,this,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1607,doc.,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1608,So,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1609,looks,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1610,like,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1611,this,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1612,approach,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1613,is,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1614,splitting,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1615,"""subgraph""","Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1616,snapshots,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1617,into,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1618,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1619,range,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1620,"buckets,","Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1621,but,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1622,seems,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1623,hard,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1624,to,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1625,scale,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1626,as,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1627,query,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1628,on,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1629,graph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1630,can,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1631,be,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1632,any,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1633,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1634,range,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1635,based,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1636,on,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1637,timestamp/epoch,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1638,(like,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1639,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1640,series,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1641,DB),"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1642,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1643,split,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1644,into,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1645,subgraph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1646,by,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1647,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1648,bucket,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1649,could,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1650,be,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1651,lots,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1652,of,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1653,duplicate,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1654,vertex,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1655,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1656,edges,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1657,in,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1658,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1659,bucket.,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1660,The,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1661,problem,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1662,I,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1663,am,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1664,currently,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1665,facing,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1666,using,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1667,TigerGraph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1668,is,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1669,-,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1670,how,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1671,to,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1672,model,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1673,geo-temporal,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1674,graph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1675,so,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1676,can,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1677,make,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1678,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1679,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1680,geo-based,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1681,query?,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1682,i.e,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1683,each,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1684,of,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1685,vertex,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1686,has,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1687,geographical,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1688,location,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1689,(like,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1690,"city,","Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1691,country),"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1692,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1693,each,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1694,vertex,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1695,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1696,edges,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1697,also,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1698,changes,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1699,in,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1700,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1701,with,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1702,its,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1703,own,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1704,timestamp,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1705,(like,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1706,a,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1707,vertex/edge,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1708,exist,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1709,in,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1710,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1711,T1,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1712,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1713,not,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1714,exist,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1715,in,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1716,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1717,T2).,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1718,There,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1719,are,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1720,numerous,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1721,use,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1722,cases,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1723,like,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1724,search,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1725,"flights,","Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1726,search,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1727,uber/lyft,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1728,"rides,","Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1729,search,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1730,social,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1731,networks,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1732,all,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1733,has,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1734,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1735,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1736,location,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1737,based,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1738,search,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1739,in,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1740,"graph,","Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1741,but,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1742,seems,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1743,unable,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1744,to,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1745,find,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1746,such,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1747,example,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1748,yet.,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1749,Is,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1750,there,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1751,good,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1752,solution,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1753,example,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1754,of,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1755,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1756,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1757,geo,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1758,location,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1759,based,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1760,graph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1761,in,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1762,TigerGraph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1763,where,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1764,we,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1765,can,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1766,apply,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1767,graph,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1768,algorithms,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1769,based,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1770,on,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1771,interested,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1772,time,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1773,range,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1774,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1775,geo,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1776,location,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1777,range?,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1778,Appreciate,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1779,for,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1780,help,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1781,and,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1782,pointers,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1783,of,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1784,examples!,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1785,eric,"Thanks a lot @Bruno for this doc.  So looks like this approach is splitting ""subgraph"" snapshots into time range buckets, but seems hard to scale as query on graph can be any time range based on timestamp/epoch (like time series DB) and split into subgraph by time bucket could be lots of duplicate vertex and edges in time bucket.

The problem I am currently facing using TigerGraph is - how to model geo-temporal graph so can make time and geo-based query?  i.e each of vertex has geographical location (like city, country) and each vertex and edges also changes in time with its own timestamp (like a vertex/edge exist in time T1 and not exist in time T2).   There are numerous use cases like search flights, search uber/lyft rides, search social networks all has time and location based search in graph, but seems unable to find such example yet.

Is there good solution example of time and geo location based graph in TigerGraph where we can apply graph algorithms based on interested time range and geo location range?   

Appreciate for help and pointers of examples!
eric",1763,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1786,Another,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1787,resource,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1788,around,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1789,TigerGraph,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1790,and,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1791,machine,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1792,learning:,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1793,https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1794,I,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1795,know,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1796,that,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1797,@rik,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1798,is,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1799,building,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1800,ML,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1801,with,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1802,Python,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1803,-,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1804,maybe,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1805,he,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1806,can,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1807,jump,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1808,in,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1809,here,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1810,when,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1811,he,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1812,has,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1813,time.,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1814,Bruno,"Another resource around TigerGraph and machine learning:
https://colab.research.google.com/drive/1-Dgl804R_4wEmH4efggCEkhcx6ATGHAU

I know that @rik is building ML with Python - maybe he can jump in here when he has time.

Bruno",1762,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1815,Hi,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1816,"@eric,","Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1817,the,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1818,best,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1819,way,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1820,to,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1821,build,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1822,timeseries,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1823,schema,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1824,using,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1825,graph,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1826,database,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1827,would,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1828,be,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1829,to,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1830,put,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1831,the,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1832,date,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1833,part,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1834,into,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1835,a,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1836,vertex,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1837,(or,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1838,multiple,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1839,"vertices,","Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1840,depending,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1841,on,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1842,your,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1843,query,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1844,preferences!),"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1845,and,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1846,to,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1847,connect,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1848,the,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1849,data,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1850,with,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1851,date,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1852,using,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1853,edges.,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1854,That,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1855,way,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1856,you,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1857,will,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1858,be,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1859,able,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1860,to,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1861,make,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1862,a,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1863,very,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1864,fast,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1865,"queries,","Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1866,much,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1867,faster,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1868,than,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1869,filtering,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1870,out,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1871,by,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1872,using,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1873,where,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1874,clause.,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1875,Please,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1876,check,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1877,slides,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1878,11,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1879,and,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1880,12,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1881,in,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1882,this,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1883,document:,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1884,https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1885,"Best,","Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1886,Bruno,"Hi @eric,

the best way to build timeseries schema using graph database would be to put the date part into a vertex (or multiple vertices, depending on your query preferences!) and to connect the data with date using edges. That way you will be able to make a very fast queries, much faster than filtering out by using where clause.

Please check slides 11 and 12 in this document:
https://drive.google.com/file/d/11YL497fWZwObR1J2qyPb7iIA55sHEiTR/view

Best,
Bruno",1761,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
1887,@Bruno,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1888,Thank,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1889,you,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1890,very,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1891,much,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1892,for,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1893,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1894,Links.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1895,I,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1896,have,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1897,seen,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1898,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1899,demo,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1900,with,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1901,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1902,graph,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1903,ML,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1904,and,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1905,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1906,issue,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1907,here,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1908,is,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1909,indeed,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1910,it,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1911,does,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1912,not,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1913,address,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1914,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1915,specific,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1916,of,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1917,our,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1918,"infrastructure,","@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1919,which,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1920,rely,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1921,on,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1922,Spark,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1923,ML.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1924,I,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1925,have,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1926,not,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1927,seen,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1928,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1929,deep,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1930,learning,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1931,one.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1932,But,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1933,"ultimately,","@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1934,what,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1935,i,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1936,am,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1937,specifically,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1938,after,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1939,is:,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1940,how,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1941,as,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1942,claimed,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1943,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1944,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1945,spark,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1946,tiger,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1947,graph,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1948,integration,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1949,"webinar,","@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1950,can,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1951,you,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1952,store,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1953,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1954,parameter,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1955,of,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1956,your,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1957,model,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1958,as,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1959,configuration,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1960,"file,","@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1961,and,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1962,then,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1963,load,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1964,it,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1965,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1966,a,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1967,query,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1968,to,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1969,do,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1970,real,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1971,time,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1972,prediction.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1973,I,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1974,did,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1975,not,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1976,see,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1977,it,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1978,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1979,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1980,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1981,Graph,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1982,ML,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1983,demo.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1984,It,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1985,is,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1986,just,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1987,that,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1988,specific,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1989,claim,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1990,that,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1991,i,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1992,would,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1993,like,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1994,to,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1995,see,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1996,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1997,"action,","@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1998,or,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
1999,have,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2000,an,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2001,explanation,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2002,of,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2003,here.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2004,Will,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2005,check,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2006,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2007,deep,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2008,learning,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2009,one,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2010,in,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2011,the,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2012,mean,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2013,time.,"@Bruno Thank you very much for the Links. 

I have seen the demo with in graph ML and the issue here is indeed it does not address the specific of our infrastructure, which rely on Spark ML. 

I have not seen the deep learning one. 


But ultimately, what i am specifically after is: how as claimed in the spark tiger graph integration webinar, can you store the parameter of your model as configuration file, and then load it in a query to do real time prediction. I did not see it in the in Graph ML demo. 

It is just that specific claim that i would like to see in action, or have an explanation of here. 

Will check the deep learning one in the mean time.",1760,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2014,Hey,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2015,"@Maatdeamon,","Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2016,there,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2017,is,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2018,a,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2019,nice,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2020,demo,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2021,of,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2022,in,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2023,database,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2024,ML,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2025,in,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2026,our,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2027,Github,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2028,ecosystem:,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2029,https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2030,"Also,","Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2031,you,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2032,can,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2033,find,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2034,Graph,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2035,Gurus,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2036,19,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2037,(deep,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2038,learning),"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2039,script,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2040,here:,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2041,https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2042,They,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2043,are,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2044,not,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2045,based,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2046,on,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2047,"Spark,","Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2048,but,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2049,I,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2050,think,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2051,you,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2052,will,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2053,get,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2054,the,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2055,idea,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2056,how,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2057,to,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2058,use,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2059,ML,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2060,with,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2061,TigerGraph.,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2062,"Best,","Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2063,Bruno,"Hey @Maatdeamon,

there is a nice demo of in database ML in our Github ecosystem:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru28_in-database_ML

Also, you can find Graph Gurus 19 (deep learning) script here:
https://github.com/tigergraph/ecosys/tree/master/demos/guru_scripts/guru19_deep_learning

They are not based on Spark, but I think you will get the idea how to use ML with TigerGraph.

Best,
Bruno",1759,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2064,No,No plans yet to support GraphQL.,1758,https://community.tigergraph.com/t/graphql-support/592
2065,plans,No plans yet to support GraphQL.,1758,https://community.tigergraph.com/t/graphql-support/592
2066,yet,No plans yet to support GraphQL.,1758,https://community.tigergraph.com/t/graphql-support/592
2067,to,No plans yet to support GraphQL.,1758,https://community.tigergraph.com/t/graphql-support/592
2068,support,No plans yet to support GraphQL.,1758,https://community.tigergraph.com/t/graphql-support/592
2069,GraphQL.,No plans yet to support GraphQL.,1758,https://community.tigergraph.com/t/graphql-support/592
2070,This,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2071,is,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2072,on,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2073,the,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2074,roadmap.,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2075,All,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2076,GSQL,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2077,commands,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2078,will,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2079,be,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2080,available,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2081,via,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2082,REST.,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2083,We,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2084,dont,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2085,have,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2086,a,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2087,confirmed,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2088,date,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2089,yet.,This is on the roadmap. All GSQL commands will be available via REST. We dont have a confirmed date yet.,1757,https://community.tigergraph.com/t/defining-a-graph-schema-via-restpp/125
2090,In,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2091,the,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2092,Following,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2093,Webinar,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2094,and,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2095,several,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2096,others,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2097,before,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2098,that,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2099,(Bad,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2100,phone,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2101,call,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2102,detection,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2103,use,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2104,"case),","In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2105,https://www.youtube.com/watch?v=bPQgRzxZeaw,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2106,(,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2107,Graph,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2108,Gurus,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2109,21:,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2110,Integrating,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2111,Real-Time,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2112,Deep-Link,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2113,Graph,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2114,Analytics,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2115,with,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2116,Spark,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2117,"AI),","In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2118,It,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2119,is,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2120,receptively,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2121,mentioned,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2122,that,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2123,models,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2124,can,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2125,be,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2126,stored,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2127,back,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2128,in,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2129,Tigergraph,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2130,for,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2131,real,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2132,time,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2133,prediction.,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2134,Do,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2135,you,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2136,have,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2137,example,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2138,of,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2139,how,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2140,this,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2141,is,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2142,done,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2143,?,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2144,An,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2145,example,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2146,of,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2147,a,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2148,model,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2149,function,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2150,or,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2151,"whatever,","In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2152,trained,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2153,in,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2154,"Spark,","In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2155,then,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2156,written,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2157,in,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2158,"TigerGraph,","In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2159,with,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2160,the,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2161,configuration,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2162,file,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2163,for,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2164,the,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2165,"parameters,","In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2166,as,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2167,explained,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2168,in,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2169,the,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2170,several,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2171,webinar,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2172,but,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2173,not,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2174,shown.,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2175,I,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2176,am,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2177,really,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2178,curious,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2179,to,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2180,get,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2181,a,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2182,sense,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2183,of,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2184,how,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2185,this,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2186,is,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2187,actually,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2188,done.,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2189,Many,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2190,thanks,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2191,![Picture,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2192,of,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2193,the,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2194,slide,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2195,of,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2196,the,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2197,webinar,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2198,where,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2199,it,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2200,is,"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2201,mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg),"In the Following Webinar and several others before that (Bad phone call detection use case), https://www.youtube.com/watch?v=bPQgRzxZeaw ( Graph Gurus 21: Integrating Real-Time Deep-Link Graph Analytics with Spark AI), It is receptively mentioned that models can be stored back in Tigergraph for real time prediction. 

Do you have example of how this is done ? An example of a model function or whatever, trained in Spark, then written in TigerGraph, with the configuration file for the parameters, as explained in the several webinar but not shown. 

I am really curious to get a sense of how this is actually done. 

Many thanks

![Picture of the slide of the webinar where it is mentioned](upload://rf8cDT2m9OlKxxne0WC4QrQgmWk.jpeg)",1756,https://community.tigergraph.com/t/storing-ml-models-in-tigergraph-from-the-spark-tigergraph-integration/614
2202,"Hi,","Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2203,I,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2204,am,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2205,newbie,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2206,here,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2207,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2208,have,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2209,been,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2210,struggling,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2211,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2212,find,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2213,examples,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2214,of,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2215,modeling,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2216,timeseries,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2217,graph,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2218,using,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2219,TigerGraphStudio.,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2220,Tried,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2221,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2222,ask,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2223,in,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2224,slack,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2225,almost,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2226,1,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2227,week,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2228,ago,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2229,no,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2230,luck.,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2231,Hope,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2232,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2233,find,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2234,help,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2235,with,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2236,any,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2237,similar,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2238,examples,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2239,here...,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2240,Like,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2241,airports,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2242,with,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2243,many,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2244,flights,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2245,at,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2246,same,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2247,or,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2248,different,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2249,time,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2250,from,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2251,source,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2252,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2253,destination,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2254,with,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2255,different,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2256,delay,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2257,or,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2258,transit,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2259,hops,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2260,of,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2261,each,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2262,"flights,","Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2263,I,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2264,have,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2265,network,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2266,traffic,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2267,data,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2268,(vertex,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2269,as,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2270,device,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2271,IP,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2272,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2273,edge,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2274,as,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2275,socket,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2276,"connections),","Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2277,so,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2278,each,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2279,directed,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2280,edge,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2281,is,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2282,source_IP:source_port,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2283,->,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2284,dest_IP:dest_port,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2285,with,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2286,delay,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2287,metrics,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2288,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2289,timestamp,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2290,of,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2291,each,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2292,event.,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2293,How,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2294,do,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2295,we,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2296,model,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2297,such,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2298,graph,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2299,(similar,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2300,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2301,"airports,","Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2302,airlines,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2303,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2304,flights,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2305,time,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2306,tables),"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2307,using,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2308,TigerGraphStudio?,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2309,If,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2310,there,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2311,is,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2312,a,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2313,way,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2314,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2315,model,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2316,such,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2317,timeseries,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2318,graph,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2319,in,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2320,"TigerGraph,","Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2321,how,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2322,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2323,query,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2324,the,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2325,graph,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2326,so,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2327,we,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2328,can,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2329,see,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2330,what,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2331,is,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2332,all,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2333,connections/path,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2334,from,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2335,given,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2336,vertex,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2337,A,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2338,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2339,remote,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2340,vertex,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2341,B,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2342,between,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2343,timestamp,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2344,t1,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2345,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2346,t2?,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2347,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2348,compare,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2349,delay,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2350,of,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2351,each,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2352,path,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2353,in,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2354,given,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2355,time,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2356,range?,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2357,I,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2358,thought,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2359,its,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2360,typical,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2361,scenario,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2362,of,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2363,graph,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2364,like,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2365,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2366,book,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2367,flights,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2368,for,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2369,travel,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2370,with,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2371,selected,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2372,dates,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2373,considering,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2374,different,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2375,time,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2376,delay,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2377,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2378,"cost,","Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2379,but,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2380,unfortunately,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2381,unable,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2382,to,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2383,find,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2384,such,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2385,modeling,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2386,examples,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2387,yet,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2388,from,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2389,graph,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2390,DB,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2391,so,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2392,not,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2393,sure,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2394,if,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2395,I,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2396,missed,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2397,something...,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2398,Appreciate,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2399,for,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2400,help,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2401,and,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2402,pointer,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2403,of,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2404,examples!,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
2405,eric,"Hi,

I am newbie here and have been struggling to find examples of modeling timeseries graph using TigerGraphStudio.  Tried to ask in slack almost 1 week ago no luck.  Hope to find help with any similar examples here...

Like airports with many flights at same or different time from source to destination with different delay or transit hops of each flights, I have network traffic data (vertex as device IP and edge as socket connections), so each directed edge is source_IP:source_port -> dest_IP:dest_port with delay metrics and timestamp of each event.

How do we model such graph (similar to airports, airlines and flights time tables) using TigerGraphStudio?  If there is a way to model such timeseries graph in TigerGraph, how to query the graph so we can see what is all connections/path from given vertex A to remote vertex B between timestamp t1 to t2?   and compare delay of each path in given time range?   

I thought its typical scenario of graph like to book flights for travel with selected dates considering different time delay and cost, but unfortunately unable to find such modeling examples yet from graph DB so not sure if I missed something...

Appreciate for help and pointer of examples!
eric",1749,https://community.tigergraph.com/t/timeseries-graph-modeling-using-tigergraph/608
